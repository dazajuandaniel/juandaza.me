# Azure DP-100 Day 3

## Deploying and Consuming Models
`Inferencing` refers to the use of a trained model to predict labels for new data on which the model has not been trained. In Azure Machine learning, you can create real-time inferencing solutions by deploying a model in a containerized platform such as `Azure Kubernetes Services (AKS)`.

### Deploying a Service
You can deploy a model as a real-time web service to the following compute targets:

    * local compute
    * an Azure Machine Learning compute instance
    * an Azure Container Instance (ACI)
    * an Azure Kubernetes Service (AKS) cluster
    * an Azure Function
    * an Internet of Things (IoT) module. 
    
Azure Machine Learning uses containers as a deployment mechanism, packaging the model and the code to use it as an image that can be deployed to a container in your chosen compute target.

To Deploy a model:

1. Register a trained model using the `register` method
```python
from azureml.core import Model

classification_model = Model.register(workspace=ws,
                       model_name='classification_model',
                       model_path='model.pkl', # local path
                       description='A classification model')
```
2. Define Inference Configuration

A model is composed of `a script to load the model` and `an environment`.

#### Entry Script

The `entry script` must contain:
    
    *init()
    *run(raw_data)

```python
import json
import joblib
import numpy as np
from azureml.core.model import Model

# Called when the service is loaded
def init():
    global model
    # Get the path to the registered model file and load it
    model_path = Model.get_model_path('classification_model')
    model = joblib.load(model_path)

# Called when a request is received
def run(raw_data):
    # Get the input data as a numpy array
    data = np.array(json.loads(raw_data)['data'])
    # Get a prediction from the model
    predictions = model.predict(data)
    # Return the predictions as any JSON serializable format
    return predictions.tolist()
``` 

#### Creating an Environment
It is possible use a `CondaDependencies` class to create a default environment, you can then add any other required packages, and then serialize the environment to a string and save it.

```python
from azureml.core.conda_dependencies import CondaDependencies

# Add the dependencies for your model
myenv = CondaDependencies()
myenv.add_conda_package("scikit-learn")

# Save the environment config as a .yml file
env_file = 'service_files/env.yml'
with open(env_file,"w") as f:
    f.write(myenv.serialize_to_string())
print("Saved dependency info in", env_file)
```

Then both script and config file can be added into one script:
```python
from azureml.core.model import InferenceConfig

classifier_inference_config = InferenceConfig(runtime= "python",
                                              source_directory = 'service_files',
                                              entry_script="score.py",
                                              conda_file="env.yml")
```

3. Define a Deployment Configuration
```python
from azureml.core.compute import ComputeTarget, AksCompute

cluster_name = 'aks-cluster'
compute_config = AksCompute.provisioning_configuration(location='eastus')
production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)
production_cluster.wait_for_completion(show_output=True)
```
Now, define the deployment configuration
```python
from azureml.core.webservice import AksWebservice

classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,
                                                              memory_gb = 1)
```
**Note:** To deploy a model to an Azure Function, you do not need to create a deployment configuration.

4. Deploy the Model
```python
from azureml.core.model import Model

service = Model.deploy(workspace=ws,
                       name = 'classifier-service',
                       models = [classification_model],
                       inference_config = classifier_inference_config,
                       deployment_config = classifier_deploy_config,
                       deployment_target = production_cluster)
service.wait_for_deployment(show_output = True)
```
### Consuming a Service
To consume a service, there are multiple possiblities

1. Use the Azure ML SDK to call a web service through the `run` method of a `WebService` object that references the deployed service.
2. Use a REST Endpoint

```python
endpoint = service.scoring_uri
print(endpoint)
```

**Authentication Methods**

* Key
* Token (JWT)

By default, authentication is disabled for ACI services, and set to key-based authentication for AKS serviceS.

### Troubleshooting a Web Service

#### Check the service

1. Check the state

```python
from azureml.core.webservice import AksWebservice

# Get the deployed service
service = AciWebservice(name='classifier-service', workspace=ws)

# Check its state
print(service.state)
```

2. Review Service Logs
```python
print(service.get_logs())
```

3. Deploy to a Local Container
Deployment and runtime errors can be easier to diagnose by deploying the service as a container in a local Docker instance

```python
from azureml.core.webservice import LocalWebservice

deployment_config = LocalWebservice.deploy_configuration(port=8890)
service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)
print(service.run(input_data = json_data))
```

## Hyperparameter Tuning
The set of hyperparameter values tried during hyperparameter tuning is known as the `search space`.

To define a search space for hyperparameter tuning, create a dictionary with the appropriate parameter expression for each named hyperparameter.

The specific values used in a hyperparameter tuning run depend on the type of `sampling` used.

**Note** Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space


```python
from azureml.train.hyperdrive import choice, normal

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': normal(10, 3)
              }
# Grid Sampling
param_sampling = GridParameterSampling(param_space)

# Random Sampling
param_sampling = RandomParameterSampling(param_space)

# Bayesian Sampling
param_sampling = BayesianParameterSampling(param_space)
```

### Early Termination Policy
The policy is evaluated at an `evaluation_interval` you specify, based on each time the target performance metric is logged. You can also set a `delay_evaluation` parameter to avoid evaluating the policy until a minimum number of iterations have been completed.

Policy types:
    
    1. Bandit: target performance metric underperforms the best run so far by a specified margin
    2. Median: where the target performance metric is worse than the median of the running averages for all runs
    3. Truncation: cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X

```python
from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy(slack_amount = 0.2,
                                        evaluation_interval=1,
                                        delay_evaluation=5)

from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,
                                                delay_evaluation=5)

from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,
                                                     evaluation_interval=1,
                                                     delay_evaluation=5)
```

### Hyperdrive
Hyperparameters can be tuned by running a Hyperdrive experiment.

Script requirements:

    * Include an argument for each hyperparameter you want to vary.

    * Log the target performance metric. This is what enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.

**Configuring and Running a Hyperdrive Experiment**
To prepare the Hyperdrive experiment, you must use a `HyperDriveConfig` object to configure the experiment run.

### Monitoring Hyperdrives
You can monitor Hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks `RunDetails` widget.



