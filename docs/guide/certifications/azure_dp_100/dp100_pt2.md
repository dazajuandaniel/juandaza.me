# Azure DP-100 Day 2

## Working with Data
In Azure Machine Learning, `datastores` are abstractions for cloud data sources

### Datastores
* Abstractions for cloud data sources
	* Azure Storage
	* Data Lake/SQL DB/Databricks
* Enable data access from workspace
	* Upload/Download
	* Remote Mount

You can access datastores directly in code by using the Azure Machine Learning SDK, and use it to upload or download data. You can also mount a datastore in an experiment in order to read or write data.

### Data in Workspaces
Every workspace has two built-in datastores (an Azure Storage blob container, and an Azure Storage file container) that are used as system storage by Azure Machine Learning. However, more custom can be added easily with SDK. Similarly, the SDK allows for management of the datastores.

```python
from azureml.core import Workspace, Datastore

ws = Workspace.from_config()

# Register a new datastore
blob_ds = Datastore.register_azure_blob_container(workspace=ws,
                                                  datastore_name='blob_data',
                                                  container_name='data_container',
                                                  account_name='az_store_acct',
                                                  account_key='123456abcde789…')
```
Azure blob and file datastores are the most commonly used. You can use the Azure Machine Learning SDK to work directly with these datastores, and to pass  `data references`  to scripts that need to access data.

To work directly with a datastore, the class `datastore` is required
```python
blob_ds.upload(src_dir='/files',
               target_path='/data/files',
               overwrite=True, show_progress=True)

blob_ds.download(target_path='downloads',
                 prefix='/data',
                 show_progress=True)
```
### Datasets
`Datasets` are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.

You can create a dataset and work with it immediately, and you can then  `register`  the dataset in the workspace to make it available for use in experiments and data processing pipelines later.
```python
from azureml.core import Dataset

blob_ds = ws.get_default_datastore()
csv_paths = [(blob_ds, 'data/files/current_data.csv'),
             (blob_ds, 'data/files/archive/*.csv')]
tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)
tab_ds = tab_ds.register(workspace=ws, name='csv_table')
```
Datasets can be  `versioned`, enabling you to track historical versions of datasets that were used in experiments, and reproduce those experiments with data in the same state.

## Compute Contexts
### Environments
Python code runs in the context of a `virtual environment` that defines the version of the Python runtime to be used as well as the installed packages available to the code. In most Python installations, packages are installed and managed in environments using **Conda** or **pip**.

Environments are encapsulated by the **Environment** class; which you can use to create environments and specify runtime configuration for an experiment. In general, Azure Machine Learning handles environment creation and package installation for you. You can specify the Conda or pip packages you need, and have Azure Machine Learning create an environment for the experiment. You can manage your own environments and register them as well.

#### Creating Environments & Registering Environments

 1. From Spec file
```python
from azureml.core import Environment

env = Environment.from_conda_specification(name='training_environment',
                                           file_path='./conda.yml')
```
 2. From Existing Conda Environment
```python
from azureml.core import Environment

env = Environment.from_existing_conda_environment(name='training_environment',
                                                  conda_environment_name='py_env')
```
 3. By Specifying Packages
```python
from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

env = Environment('training_environment')
deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy'],
                                pip_packages=['azureml-defaults'])
env.python.conda_dependencies = deps
```
After you've created an environment, you can register it in your workspace and reuse it for future experiments that have the same Python dependencies.
```python
env.register(workspace=ws)
```
To retrieve and use an environment
```python
from azureml.core import Environment
from azureml.train.estimator import Estimator

training_env = Environment.get(workspace=ws, name='training_environment')
estimator = Estimator(source_directory='experiment_folder'
                      entry_script='training_script.py',
                      compute_target='local',
                      environment_definition=training_env)
```
Curated environments are registered in all Azure Machine Learning workspaces with a name that begins  **AzureML-**.

**Note**: You can't register your own environments with an “AzureML-” prefix.

### Compute Targets
`Compute Targets` are physical or virtual computers on which experiments are run.

	* Local compute: This runs the experiment on the same compute target as the code used to initiate the experiment
	* Compute Clusters: For experiment with high scalability requirements, you can use Azure Machine Learning compute clusters
	* Inference Clusters: To deploy trained models as production services, you can use Azure Machine Learning inference clusters
	* Attached Compute: If you already use an Azure-based compute environment for data science, such as a virtual machine or an Azure Databricks cluster, you can attach it to your Azure Machine Learning workspace and use it as a compute target for certain types of workload.

#### Using Compute Targets
To use a particular compute target, you can specify it in the appropriate parameter for an experiment run configuration or estimator
```python
from azureml.core import Environment
from azureml.train.estimator import Estimator

compute_name = 'aml-cluster'

training_env = Environment.get(workspace=ws, name='training_environment')

estimator = Estimator(source_directory='experiment_folder',
                      entry_script='training_script.py',
                      environment_definition=training_env,
                      compute_target=compute_name)
```
## Pipelines
In Azure Machine Learning, a `pipeline` is a workflow of machine learning tasks in which each task is implemented as a `step`. Steps can be arranged sequentially or in parallel, enabling you to build sophisticated flow logic to orchestrate machine learning operations. Each step can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal.

A pipeline can be executed as a process by running the pipeline as an experiment. Each step in the pipeline runs on its allocated compute target as part of the overall experiment run.

**Note:** You can publish a pipeline as a REST endpoint, enabling client applications to initiate a pipeline run. You can also define a schedule for a pipeline, and have it run automatically at periodic intervals
### Pipeline Steps
To create a pipeline, you must first define each step and then create a pipeline that includes the steps. The specific configuration of each step depends on the step type. These the sames as the modules seen in the Drag&Drop interface.

For example, pipeline for **PythonScriptStep**:
```python
from azureml.pipeline.steps import PythonScriptStep, EstimatorStep

# Step to run a Python script
step1 = PythonScriptStep(name = 'prepare data',
                         source_directory = 'scripts',
                         script_name = 'data_prep.py',
                         compute_target = 'aml-cluster',
                         runconfig = run_config)

# Step to run an estimator
step2 = EstimatorStep(name = 'train model',
                      estimator = sk_estimator,
                      compute_target = 'aml-cluster')
```
Once defined, the pipeline can be built 
```python
from azureml.pipeline.core import Pipeline
from azureml.core import Experiment

# Construct the pipeline
train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])

# Create an experiment and run the pipeline
experiment = Experiment(workspace = ws, name = 'training-pipeline')
pipeline_run = experiment.submit(train_pipeline)
```
#### Passing Data Between Steps
Using the `PipelineData` object you can pass data from one step to another, this is a special kind of `DataReference`,
```python
from azureml.pipeline.core import PipelineData
from azureml.pipeline.steps import PythonScriptStep, EstimatorStep

# Get a dataset for the initial data
raw_ds = Dataset.get_by_name(ws, 'raw_dataset')

# Define a PipelineData object to pass data between steps
data_store = ws.get_default_datastore()
prepped_data = PipelineData('prepped',  datastore=data_store)

# Step to run a Python script
step1 = PythonScriptStep(name = 'prepare data',
                         source_directory = 'scripts',
                         script_name = 'data_prep.py',
                         compute_target = 'aml-cluster',
                         runconfig = run_config,
                         # Specify dataset as initial input
                         inputs=[raw_ds.as_named_input('raw_data')],
                         # Specify PipelineData as output
                         outputs=[prepped_data],
                         # Also pass as data reference to script
                         arguments = ['--folder', prepped_data])

# Step to run an estimator
step2 = EstimatorStep(name = 'train model',
                      estimator = sk_estimator,
                      compute_target = 'aml-cluster',
                      # Specify PipelineData as input
                      inputs=[prepped_data],
                      # Pass as data reference to estimator script
                      estimator_entry_script_arguments=['--folder', prepped_data])
```

#### Pipeline Step Reuse
Pipelines with multiple long-running steps can take a significant time to complete, so Azure Machine Learning includes some default caching and reuse features to reduce this time. By default the step output from a previous pipeline run is reused without re-running the step as long as the script, source directory, and other parameters for the step have not changed. This can significantly reduce the time it takes to run a pipeline; however it can lead to stale results when changes to downstream data sources have not been accounted for.

To control reuse for an individual step, you can set the `allow_reuse` parameter in the step configuration, like this:

```python
step1 = PythonScriptStep(name = 'prepare data',
                         source_directory = 'scripts',
                         script_name = 'data_prep.py',
                         compute_target = 'aml-cluster',
                         runconfig = run_config,
                         inputs=[raw_ds.as_named_input('raw_data')],
                         outputs=[prepped_data],
                         arguments = ['--folder', prepped_data]),
                         # Disable step reuse
                         allow_reuse = False)
```
This can be overwritten by forcing all steps to be reused.
```python
pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)
```