# Azure DP-100 Day 1

## Azure ML Studio Concepts

#### Compute Resources
This is where you'll manage all the compute targets for your data science activities.
	
	* Compute instances: Add a new server
	* Compute clusters: Add a new cluster
	* Interface clusters: This is where you can create and manage compute targets on which to deploy your trained models as web services for client applications to consume
	* Attached compute: This is where you could attach a virtual machine or Databricks cluster that exists outside of your workspace

#### Datastores
Azure ML workspace already includes two datastores based on the Azure Storage account that was created along with the workspace. These are used to store notebooks, configuration files, and data

#### Datasets
Datasets represent specific data files or tables that you plan to work with in Azure ML. Datasets have handy options available to have a quick glance at exploring the data:
	
	1. Details tab: Provides general metadata details
	2. Explore tab: Brief overview of data content, quick plots, etc
	3. Consume tab: Code details on how to use the data
	4. Models

#### Azure ML SDK
Provides libraries to interact programatically with Azure

#### Visual Studio Codespace
A Visual Studio Codespace is a hosted instance of Visual Studio Code that you can use in a web browser. Visual Studio Code is a general code editing environment, with support for various programming languages through the installation of  _extensions_. To work with Python, you'll need the Microsoft Python extension, which was installed for you along with some commonly used Python packages when you created this environment from the  **DP100**  repo.

If you plan to work with Azure Machine Learning in a Visual Studio codespace (or a local installation of Visual Studio Code), the Azure Machine Learning extension can help make it easier to work with resources in your workspace without needing to switch between your code development environment and the Azure Machine Learning studio web interface.

### Azure ML Designer
The  `Designer`  interface provides a drag & drop environment in which you can define a workflow, or  _pipeline_  of data ingestion, transformation, and model training modules to create a machine learning model. You can then publish this pipeline as a web service that client applications can use for  `inferencing`  (generating predictions from new data).

#### Pipeline:
A dataflow for training a machine learning model. It includes all the steps required to process and train. Each step is independently executed.

#### Training, Scoring & Evaluating Models
General steps:
1. Use an algorithm module to specify type of module
2. Train the model by fitting the data -> For supervised learning algorithms, you must use the **Train Model** module and specify the label to be predicted from the features in the training data. For unsupervised clustering, you must use the **Train Clustering Model** module.
3. Split the data into Train/Test using the relevant module
4. Evaluate the model

**Note:**

1. You could try some different classification algorithms and compare the results (you can connect the outputs of the **Split Data** module to multiple **Train Model** and **Score Model** modules, and you can connect a second scored model to the **Evaluate Model** module to see a side-by-side comparison)
2. You can't evaluate an unsupervised clustering model using the **Evaluate Model** module. To evaluate a clustering model, you can use the model to assign data to clusters and then use custom code to compare cluster separation based on the distance to cluster center values produced for each data point, or calculate and plot a `Principal Component Analysis` (PCA) to show the separation of clusters based on two principal components.

#### Code Modules
There are multiple OOTB code modules, but custom modules are accepted, these include the following:

	* Apply SQL Transformation: A SQL statement based on the SQLite implementation of the SQL language.

	* Execute Python Script: Use this module to run a custom Python function that processes one or two input dataframes, and returns one or two output dataframes.

	* Create Python Model: You can use this module to implement code for a custom model in place of the built-in algorithms provided in the designer. Your code must implement a class named  AzureMLModel  with init, train, and predict  methods.

	* Execute R Script: Use this module to run a custom R function that processes one or two input dataframes, and returns one or two output dataframes.

#### Interface Pipeline

Having trained a model using a  `training pipeline`, you can use it to create an  `inference pipeline`  for either real-time or batch prediction. It encapsulates the steps required to use it as a web service.

Once deployed, it can be consumed quite easily.
```python
import urllib.request
import json
import os
import ssl

def allowSelfSignedHttps(allowed):
    # bypass the server certificate verification on client side
    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):
        ssl._create_default_https_context = ssl._create_unverified_context

allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.

data = {
    "Inputs": {
          "input0":
          [
              {
                    'PatientID': "1882185",
                    'Pregnancies': "9",
                    'PlasmaGlucose': "104",
                    'DiastolicBloodPressure': "51",
                    'TricepsThickness': "7",
                    'SerumInsulin': "24",
                    'BMI': "27.36983156",
                    'DiabetesPedigree': "1.35047",
                    'Age': "43",
              },
          ],
    },
    "GlobalParameters":  {
    }
}

body = str.encode(json.dumps(data))

url = 'http://10.0.0.1:80/api/v1/service/diabetes_predictor/score'
api_key = 'a1234567890x' # Replace this with the API key for the web service
headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}

req = urllib.request.Request(url, body, headers)

try:
    response = urllib.request.urlopen(req)

    result = response.read()
    print(result)
except urllib.error.HTTPError as error:
    print("The request failed with status code: " + str(error.code))

    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure
    print(error.info())
    print(json.loads(error.read().decode("utf8", 'ignore')))
```


### Experiments
In Azure Machine Learning, an `experiment`  is used to run a script or a pipeline, and usually generates outputs and records metrics. An experiment can be run multiple times, with different data, code, or settings; and Azure Machine Learning tracks each run, enabling you to view run history and compare results for each run.

The `run context` is used to initialize and end the experiment run tracked by AzureML
```python
from azureml.core import Experiment

# create an experiment variable
experiment = Experiment(workspace = ws, name = "my-experiment")

# start the experiment
run = experiment.start_logging()

# experiment code goes here

# end the experiment
run.complete()
```
Experiments are most useful when they produce metrics and outputs that can be tracked across runs.

Every experiment generates log files that include the messages that would be written to the terminal during interactive execution. This enables you to use simple print statements to write messages to the log. However, if you want to record named metrics for comparison across runs, you can do so by using the **Run** object; which provides a range of logging functions specifically for this purpose.

```python
from azureml.core import Experiment
import pandas as pd

# Create an Azure ML experiment in your workspace
experiment = Experiment(workspace = ws, name = 'my-experiment')

# Start logging data from the experiment
run = experiment.start_logging()

# load the dataset and count the rows
data = pd.read_csv('data.csv')
row_count = (len(data))

# Log the row count
run.log('observations', row_count)

# Complete the experiment
run.complete()
```
You can view the metrics logged by an experiment run in Azure Machine Learning studio or by using the **RunDetails** widget in a notebook.

In addition to logging metrics, an experiment can generate output files. You can run an experiment inline using the **start_logging** method of the **Experiment** object, but it's more common to encapsulate the experiment logic in a script and run the script as an experiment. An experiment script is just a Python code file that contains the code you want to run in the experiment. To run a script as an experiment, you must define a `run configuration` that defines the Python environment in which the script will be run, and a `script run configuration` that associates the run environment with the script. These are implemented by using the **RunConfiguration** and **ScriptRunConfig** objects.

### Estimators
Azure Machine Learning provides a higher level abstraction called an **Estimator** that encapsulates a run configuration and a script configuration in a single object, and for which there are pre-defined, framework-specific variants that already include the package dependencies for common machine learning frameworks such as Scikit-Learn, PyTorch, and Tensorflow

You can use a generic  **Estimator**  class to define a run configuration for a training script like this:
```python
from azureml.train.estimator import Estimator
from azureml.core import Experiment

# Create an estimator
estimator = Estimator(source_directory='experiment_folder',
                      entry_script='training_script.py'
                      compute_target='local',
                      conda_packages=['scikit-learn']
                      )

# Create and run an experiment
experiment = Experiment(workspace = ws, name = 'training_experiment')
run = experiment.submit(config=estimator)
```
#### Using Script Parameters
You can increase the flexibility of script-based experiments by using parameters to set variables in the script. To use parameters in a script, you must use a library such as **argparse** to read the arguments passed to the script and assign them to variables.

```python
# Set regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)
args = parser.parse_args()
reg = args.reg
```
#### Retrieving/Registering a Trained Model
After running an experiment that trains a model you can use a reference to the  **Run**  object to retrieve its outputs, including the trained model.
```python
# "run" is a reference to a completed experiment run

# List the files generated by the experiment
for file in run.get_file_names():
    print(file)

# Download a named file
run.download_file(name='outputs/model.pkl', output_file_path='model.pkl')
```

```python
from azureml.core import Model

model = Model.register(workspace=ws,
                       model_name='classification_model',
                       model_path='model.pkl', # local path
                       description='A classification model',
                       tags={'dept': 'sales'},
                       model_framework=Model.Framework.SCIKITLEARN,
                       model_framework_version='0.20.3')
```