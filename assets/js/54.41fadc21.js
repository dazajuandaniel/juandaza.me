(window.webpackJsonp=window.webpackJsonp||[]).push([[54],{412:function(t,e,a){"use strict";a.r(e);var n=a(42),s=Object(n.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"deep-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deep-learning"}},[t._v("#")]),t._v(" Deep Learning")]),t._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#ann-general-concepts"}},[t._v("ANN General Concepts")]),a("ul",[a("li",[a("a",{attrs:{href:"#multilayer-perceptron-backpropagation"}},[t._v("Multilayer Perceptron & Backpropagation")])])])]),a("li",[a("a",{attrs:{href:"#important-concepts"}},[t._v("Important Concepts")])]),a("li",[a("a",{attrs:{href:"#resources"}},[t._v("Resources")])])])]),a("p"),t._v(" "),a("h2",{attrs:{id:"ann-general-concepts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ann-general-concepts"}},[t._v("#")]),t._v(" ANN General Concepts")]),t._v(" "),a("h3",{attrs:{id:"multilayer-perceptron-backpropagation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multilayer-perceptron-backpropagation"}},[t._v("#")]),t._v(" Multilayer Perceptron & Backpropagation")]),t._v(" "),a("p",[a("strong",[t._v("MLP")])]),t._v(" "),a("p",[t._v("MLP -> composed of one input layer, one or more TLU (hidden layers), and one final TLU (output layer).")]),t._v(" "),a("p",[a("strong",[t._v("Backpropagation")])]),t._v(" "),a("ul",[a("li",[t._v("Handles one mini-batch at a time, goes through the full training set multiple times (called "),a("em",[t._v("epoch")]),t._v(")")]),t._v(" "),a("li",[t._v("Computes the "),a("em",[t._v("forward pass")]),t._v(" -> Computes the output of all neurons in the first layer, then it passes the result to the next layer and so on. Like making predictions but results are preserved")]),t._v(" "),a("li",[t._v("Calculates the network's output error")]),t._v(" "),a("li",[t._v("Then, it computes how much each output connection contributed to the error")]),t._v(" "),a("li",[t._v("Measures how much of these error contributions came from each connection in the layer below, working backwards.")]),t._v(" "),a("li",[t._v("Finally, it does Gradient Descent.")])]),t._v(" "),a("p",[a("strong",[t._v("Regression MLP")])]),t._v(" "),a("p",[t._v("Use single output neuron or an output neuron per dimension.")]),t._v(" "),a("ul",[a("li",[t._v("Guarantee Output is > 0 -> ReLU Activation function or SoftPlus")]),t._v(" "),a("li",[t._v("Guarantee output is in range (a,b) -> Logistic or Hyperbolic tangent and scale.")])]),t._v(" "),a("p",[a("em",[t._v("Typical Regression MLP")])]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("Hyperparameter")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Typical Value")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# input neurons")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# hidden layers")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# neurons per hidden layer")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# output neurons")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1 per dimension")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("Hidden activation")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("ReLU/SELU")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("Output activation")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("None, ReLU/SoftPlus/Logistic/tanh")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("Loss function")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("MSE or MAE/Huber (if outliers)")])])])]),t._v(" "),a("p",[a("strong",[t._v("Classification MLP")])]),t._v(" "),a("ul",[a("li",[t._v("Binary -> Single output Neuron using logistic")]),t._v(" "),a("li",[t._v("Multilabel Binary -> One output neuron per positive label")]),t._v(" "),a("li",[t._v("Multiclass -> One output Neuron per class")])]),t._v(" "),a("p",[a("em",[t._v("Typical Classification MLP")])]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("Hyperparameter")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Binary")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Multilabel")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Multiclass")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# input neurons")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# hidden layers")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# neurons per hidden layer")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("# output neurons")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1 per label")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("1 per class")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("Output activation")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Logistic")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Logistic")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("SoftMax")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("Loss function")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")])])])]),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("In Keras")]),t._v(" "),a("p",[t._v("sparse_categorical_crossentropy -> Sparse labels")]),t._v(" "),a("p",[t._v("categorical_crossentropy -> One target probability per class")]),t._v(" "),a("p",[t._v("sigmoid (logistic) -> Binary classification")])]),t._v(" "),a("p",[a("strong",[t._v("Other Topologies/Architectures")])]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://arxiv.org/abs/1606.07792",target:"_blank",rel:"noopener noreferrer"}},[a("em",[t._v("Wide & Deep")]),a("OutboundLink")],1),t._v(" -> Non sequential neural network")])]),t._v(" "),a("h2",{attrs:{id:"important-concepts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#important-concepts"}},[t._v("#")]),t._v(" Important Concepts")]),t._v(" "),a("p",[a("strong",[t._v("Vanishing Gradient Problem")])]),t._v(" "),a("p",[t._v("Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layersâ€™ connection weights virtually unchanged, and training never converges to a good solution.")]),t._v(" "),a("p",[t._v("A solution is "),a("code",[t._v("Glorot Initialization")]),t._v(" or "),a("code",[t._v("He Initialization")]),t._v(".")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_initializer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"he_normal"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LeakyReLU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),a("p",[a("strong",[t._v("Batch Normalization")])]),t._v(" "),a("p",[t._v("Consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BatchNormalization"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("strong",[t._v("Regularization")])]),t._v(" "),a("p",[a("strong",[t._v("L1 L2")])]),t._v(" "),a("p",[a("code",[t._v("L1 & L2")]),t._v(" to constrain a neural networkâ€™s connection weights or get a sparse model.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("layer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"elu"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           kernel_initializer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"he_normal"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           kernel_regularizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("regularizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("l2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("a",{attrs:{href:"https://jmlr.org/papers/v15/srivastava14a.html",target:"_blank",rel:"noopener noreferrer"}},[a("strong",[t._v("Dropout")]),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("At every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily "),a("code",[t._v("dropped out")]),t._v(", meaning it will be entirely ignored during this training step, but it may be active during the next step.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])])]),a("h2",{attrs:{id:"resources"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#resources"}},[t._v("#")]),t._v(" Resources")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Loss Functions"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=s.exports}}]);