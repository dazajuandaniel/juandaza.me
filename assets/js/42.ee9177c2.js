(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{399:function(t,a,s){"use strict";s.r(a);var e=s(42),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"azure-dp-100-module-4"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#azure-dp-100-module-4"}},[t._v("#")]),t._v(" Azure DP-100 Module 4")]),t._v(" "),s("h2",{attrs:{id:"datastores"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#datastores"}},[t._v("#")]),t._v(" Datastores")]),t._v(" "),s("p",[t._v("A "),s("code",[t._v("datastore")]),t._v(" is an abstraction for cloud data sources:")]),t._v(" "),s("ul",[s("li",[t._v("Azure Storage")]),t._v(" "),s("li",[t._v("Azure Data Lake")]),t._v(" "),s("li",[t._v("Azure SQL DB")]),t._v(" "),s("li",[t._v("Databricks file system")]),t._v(" "),s("li",[t._v("Azure Database for PostgreSQL")]),t._v(" "),s("li",[t._v("Azure Database for MySQL")]),t._v(" "),s("li",[t._v("Azure Blob Container")])]),t._v(" "),s("p",[t._v("Data stores can be accessed via the SDK, or mounted in an experiment in order to read/write data.")]),t._v(" "),s("h3",{attrs:{id:"adding-datastores-to-a-workspace"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#adding-datastores-to-a-workspace"}},[t._v("#")]),t._v(" Adding Datastores to a Workspace")]),t._v(" "),s("p",[t._v("Every workspace has 2 built-in datasores by default (Storage Blob Containter & Azure Storaga File Container), however, you can add external ones since data might be there.")]),t._v(" "),s("p",[s("strong",[t._v("Notes:")])]),t._v(" "),s("ul",[s("li",[t._v("It is a good idea to change the default datastore")]),t._v(" "),s("li",[t._v("Parquet format is generally better for performance")]),t._v(" "),s("li",[s("em",[t._v("Premium")]),t._v(" level storage in Azure Blob is usually better for performance of large files but more costly.")])]),t._v(" "),s("h4",{attrs:{id:"registering-a-datastore"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#registering-a-datastore"}},[t._v("#")]),t._v(" Registering a Datastore")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Workspace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Datastore\n\nws "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Workspace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Register a new datastore")]),t._v("\nblob_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datastore"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register_azure_blob_container"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  datastore_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blob_data'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  container_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_container'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  account_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'az_store_acct'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  account_key"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'123456abcde789â€¦'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# View Datastores")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ds_name "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datastores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Reference a datastore")]),t._v("\nblob_store "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datastore"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" datastore_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blob_data'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Use the Workspace.get_default_datastore() method to get the default datastore.")]),t._v(" "),s("h3",{attrs:{id:"using-datastores"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-datastores"}},[t._v("#")]),t._v(" Using Datastores")]),t._v(" "),s("p",[t._v("The "),s("code",[t._v("datastore")]),t._v(" class allows working directly with a datastore to upload or download data.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("upload"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("src_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               target_path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/data/files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               overwrite"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" show_progress"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("If you want to use a datastore in a script, you have to pass the data reference as an argument. It can be:")]),t._v(" "),s("ul",[s("li",[t._v("Download")]),t._v(" "),s("li",[t._v("Upload")]),t._v(" "),s("li",[t._v("Mount")])]),t._v(" "),s("p",[t._v("To access a path in a datastore in an experiment script, you must create a data reference and pass it to the script as a parameter. The script can then read data from the data reference parameter just like a local file path.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Configure the data reference to be passed to the Estimator")]),t._v("\ndata_ref "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_download"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path_on_compute"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nestimator "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SKLearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source_directory"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'experiment_folder'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    entry_script"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_script.py'")]),t._v("\n                    compute_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    script_params "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data_folder'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" data_ref"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Usage in the script")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" argparse\n\nparser "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" argparse"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ArgumentParser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data_folder'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dest"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_folder'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nargs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parser"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_files "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("listdir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_folder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"datasets"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#datasets"}},[t._v("#")]),t._v(" Datasets")]),t._v(" "),s("p",[t._v("They are versioned packaged data objects.\nThere are two different types:")]),t._v(" "),s("ol",[s("li",[t._v("Tabular (structured)")]),t._v(" "),s("li",[t._v("File (unstructured)")])]),t._v(" "),s("p",[t._v("You can create datasets from individual files or multiple file paths. The paths can include wildcards (for example, /files/*.csv) making it possible to encapsulate data from a large number of files in a single dataset.")]),t._v(" "),s("h3",{attrs:{id:"creating-a-tabular-dataset"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-tabular-dataset"}},[t._v("#")]),t._v(" Creating a Tabular Dataset")]),t._v(" "),s("p",[t._v("Using the SDK, you can use the "),s("code",[t._v("from_delimited_files")]),t._v(" method in the "),s("code",[t._v("Dataset")]),t._v(" class.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\nblob_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_default_datastore"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncsv_paths "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/current_data.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/archive/*.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntab_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tabular"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_delimited_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("csv_paths"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntab_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tab_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv_table'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"creating-a-file-dataset"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-file-dataset"}},[t._v("#")]),t._v(" Creating a File Dataset")]),t._v(" "),s("p",[t._v("Using the SDK, you can use the "),s("code",[t._v("from_files")]),t._v(" method in the "),s("code",[t._v("Dataset")]),t._v(" class.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\nblob_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_default_datastore"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.jpg'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" file_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("To retrieve a dataset you can do one of the following:")]),t._v(" "),s("ol",[s("li",[t._v("Use the "),s("code",[t._v("Datasets")]),t._v(" dictionary attribute of the Workspace object")]),t._v(" "),s("li",[t._v("Use the "),s("code",[t._v("get_by_name")]),t._v(" or "),s("code",[t._v("get_by_id")]),t._v(" of the "),s("code",[t._v("Dataset")]),t._v(" class.")])]),t._v(" "),s("h3",{attrs:{id:"reading-data-from-datasets"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#reading-data-from-datasets"}},[t._v("#")]),t._v(" Reading Data from Datasets")]),t._v(" "),s("p",[t._v("Data can be read directly from a dataset or passed as a named input to a script configuration.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Tabular")]),t._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tab_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_pandas_dataframe"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Files")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" file_path "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" file_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_path\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# As Scripts")]),t._v("\nestimator "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SKLearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" source_directory"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'experiment_folder'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     entry_script"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_script.py'")]),t._v("\n                     compute_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     inputs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tab_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_named_input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv_data'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     pip_packages"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'azureml-dataprep[pandas]'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("strong",[t._v("Note")]),t._v(" Since we are passing the dataset as an argument, we nned the "),s("code",[t._v("azureml-dataprep[pandas]")]),t._v(" package installed.")]),t._v(" "),s("h3",{attrs:{id:"data-versioning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-versioning"}},[t._v("#")]),t._v(" Data Versioning")]),t._v(" "),s("p",[t._v("Data can be versioned by using the "),s("code",[t._v("create_new_version")]),t._v(" argument in the "),s("code",[t._v("register")]),t._v(" function")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("img_paths "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.jpg'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.png'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nfile_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_files"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("img_paths"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" file_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" create_new_version"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# To retrieve a specific version")]),t._v("\nimg_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_by_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" version"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=n.exports}}]);