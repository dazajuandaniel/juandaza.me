(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{334:function(t,e,a){"use strict";a.r(e);var n=a(14),s=Object(n.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"deep-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#deep-learning"}},[t._v("#")]),t._v(" Deep Learning")]),t._v(" "),e("p"),e("div",{staticClass:"table-of-contents"},[e("ul",[e("li",[e("a",{attrs:{href:"#ann-general-concepts"}},[t._v("ANN General Concepts")]),e("ul",[e("li",[e("a",{attrs:{href:"#multilayer-perceptron-backpropagation"}},[t._v("Multilayer Perceptron & Backpropagation")])])])]),e("li",[e("a",{attrs:{href:"#important-concepts"}},[t._v("Important Concepts")])]),e("li",[e("a",{attrs:{href:"#resources"}},[t._v("Resources")])])])]),e("p"),t._v(" "),e("h2",{attrs:{id:"ann-general-concepts"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ann-general-concepts"}},[t._v("#")]),t._v(" ANN General Concepts")]),t._v(" "),e("h3",{attrs:{id:"multilayer-perceptron-backpropagation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#multilayer-perceptron-backpropagation"}},[t._v("#")]),t._v(" Multilayer Perceptron & Backpropagation")]),t._v(" "),e("p",[e("strong",[t._v("MLP")])]),t._v(" "),e("p",[t._v("MLP -> composed of one input layer, one or more TLU (hidden layers), and one final TLU (output layer).")]),t._v(" "),e("p",[e("strong",[t._v("Backpropagation")])]),t._v(" "),e("ul",[e("li",[t._v("Handles one mini-batch at a time, goes through the full training set multiple times (called "),e("em",[t._v("epoch")]),t._v(")")]),t._v(" "),e("li",[t._v("Computes the "),e("em",[t._v("forward pass")]),t._v(" -> Computes the output of all neurons in the first layer, then it passes the result to the next layer and so on. Like making predictions but results are preserved")]),t._v(" "),e("li",[t._v("Calculates the network's output error")]),t._v(" "),e("li",[t._v("Then, it computes how much each output connection contributed to the error")]),t._v(" "),e("li",[t._v("Measures how much of these error contributions came from each connection in the layer below, working backwards.")]),t._v(" "),e("li",[t._v("Finally, it does Gradient Descent.")])]),t._v(" "),e("p",[e("strong",[t._v("Regression MLP")])]),t._v(" "),e("p",[t._v("Use single output neuron or an output neuron per dimension.")]),t._v(" "),e("ul",[e("li",[t._v("Guarantee Output is > 0 -> ReLU Activation function or SoftPlus")]),t._v(" "),e("li",[t._v("Guarantee output is in range (a,b) -> Logistic or Hyperbolic tangent and scale.")])]),t._v(" "),e("p",[e("em",[t._v("Typical Regression MLP")])]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"center"}},[t._v("Hyperparameter")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("Typical Value")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# input neurons")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# hidden layers")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# neurons per hidden layer")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# output neurons")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1 per dimension")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("Hidden activation")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("ReLU/SELU")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("Output activation")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("None, ReLU/SoftPlus/Logistic/tanh")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("Loss function")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("MSE or MAE/Huber (if outliers)")])])])]),t._v(" "),e("p",[e("strong",[t._v("Classification MLP")])]),t._v(" "),e("ul",[e("li",[t._v("Binary -> Single output Neuron using logistic")]),t._v(" "),e("li",[t._v("Multilabel Binary -> One output neuron per positive label")]),t._v(" "),e("li",[t._v("Multiclass -> One output Neuron per class")])]),t._v(" "),e("p",[e("em",[t._v("Typical Classification MLP")])]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"center"}},[t._v("Hyperparameter")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("Binary")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("Multilabel")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("Multiclass")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# input neurons")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("One per input feature Value")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# hidden layers")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1-5")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# neurons per hidden layer")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("10-100")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("# output neurons")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1 per label")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("1 per class")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("Output activation")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("Logistic")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("Logistic")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("SoftMax")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"center"}},[t._v("Loss function")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("Cross entropy")])])])]),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("In Keras")]),t._v(" "),e("p",[t._v("sparse_categorical_crossentropy -> Sparse labels")]),t._v(" "),e("p",[t._v("categorical_crossentropy -> One target probability per class")]),t._v(" "),e("p",[t._v("sigmoid (logistic) -> Binary classification")])]),t._v(" "),e("p",[e("strong",[t._v("Other Topologies/Architectures")])]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://arxiv.org/abs/1606.07792",target:"_blank",rel:"noopener noreferrer"}},[e("em",[t._v("Wide & Deep")]),e("OutboundLink")],1),t._v(" -> Non sequential neural network")])]),t._v(" "),e("h2",{attrs:{id:"important-concepts"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#important-concepts"}},[t._v("#")]),t._v(" Important Concepts")]),t._v(" "),e("p",[e("strong",[t._v("Vanishing Gradient Problem")])]),t._v(" "),e("p",[t._v("Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution.")]),t._v(" "),e("p",[t._v("A solution is "),e("code",[t._v("Glorot Initialization")]),t._v(" or "),e("code",[t._v("He Initialization")]),t._v(".")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_initializer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"he_normal"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LeakyReLU"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("alpha"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("p",[e("strong",[t._v("Batch Normalization")])]),t._v(" "),e("p",[t._v("Consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BatchNormalization"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[e("strong",[t._v("Regularization")])]),t._v(" "),e("p",[e("strong",[t._v("L1 L2")])]),t._v(" "),e("p",[e("code",[t._v("L1 & L2")]),t._v(" to constrain a neural network’s connection weights or get a sparse model.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("layer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dense"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"elu"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           kernel_initializer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"he_normal"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           kernel_regularizer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("regularizers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("l2"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[e("a",{attrs:{href:"https://jmlr.org/papers/v15/srivastava14a.html",target:"_blank",rel:"noopener noreferrer"}},[e("strong",[t._v("Dropout")]),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("At every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily "),e("code",[t._v("dropped out")]),t._v(", meaning it will be entirely ignored during this training step, but it may be active during the next step.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    keras"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rate"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])])]),e("h2",{attrs:{id:"resources"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#resources"}},[t._v("#")]),t._v(" Resources")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Loss Functions"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=s.exports}}]);