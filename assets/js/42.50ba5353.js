(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{317:function(t,a,s){"use strict";s.r(a);var e=s(14),n=Object(e.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"azure-dp-100-module-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#azure-dp-100-module-4"}},[t._v("#")]),t._v(" Azure DP-100 Module 4")]),t._v(" "),a("h2",{attrs:{id:"datastores"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#datastores"}},[t._v("#")]),t._v(" Datastores")]),t._v(" "),a("p",[t._v("A "),a("code",[t._v("datastore")]),t._v(" is an abstraction for cloud data sources:")]),t._v(" "),a("ul",[a("li",[t._v("Azure Storage")]),t._v(" "),a("li",[t._v("Azure Data Lake")]),t._v(" "),a("li",[t._v("Azure SQL DB")]),t._v(" "),a("li",[t._v("Databricks file system")]),t._v(" "),a("li",[t._v("Azure Database for PostgreSQL")]),t._v(" "),a("li",[t._v("Azure Database for MySQL")]),t._v(" "),a("li",[t._v("Azure Blob Container")])]),t._v(" "),a("p",[t._v("Data stores can be accessed via the SDK, or mounted in an experiment in order to read/write data.")]),t._v(" "),a("h3",{attrs:{id:"adding-datastores-to-a-workspace"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#adding-datastores-to-a-workspace"}},[t._v("#")]),t._v(" Adding Datastores to a Workspace")]),t._v(" "),a("p",[t._v("Every workspace has 2 built-in datasores by default (Storage Blob Containter & Azure Storaga File Container), however, you can add external ones since data might be there.")]),t._v(" "),a("p",[a("strong",[t._v("Notes:")])]),t._v(" "),a("ul",[a("li",[t._v("It is a good idea to change the default datastore")]),t._v(" "),a("li",[t._v("Parquet format is generally better for performance")]),t._v(" "),a("li",[a("em",[t._v("Premium")]),t._v(" level storage in Azure Blob is usually better for performance of large files but more costly.")])]),t._v(" "),a("h4",{attrs:{id:"registering-a-datastore"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#registering-a-datastore"}},[t._v("#")]),t._v(" Registering a Datastore")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Workspace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Datastore\n\nws "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Workspace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Register a new datastore")]),t._v("\nblob_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datastore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register_azure_blob_container"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  datastore_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blob_data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  container_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_container'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  account_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'az_store_acct'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                                  account_key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'123456abcde789â€¦'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# View Datastores")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ds_name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datastores"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Reference a datastore")]),t._v("\nblob_store "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Datastore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" datastore_name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blob_data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("Use the Workspace.get_default_datastore() method to get the default datastore.")]),t._v(" "),a("h3",{attrs:{id:"using-datastores"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-datastores"}},[t._v("#")]),t._v(" Using Datastores")]),t._v(" "),a("p",[t._v("The "),a("code",[t._v("datastore")]),t._v(" class allows working directly with a datastore to upload or download data.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("upload"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("src_dir"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               target_path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/data/files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               overwrite"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" show_progress"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("If you want to use a datastore in a script, you have to pass the data reference as an argument. It can be:")]),t._v(" "),a("ul",[a("li",[t._v("Download")]),t._v(" "),a("li",[t._v("Upload")]),t._v(" "),a("li",[t._v("Mount")])]),t._v(" "),a("p",[t._v("To access a path in a datastore in an experiment script, you must create a data reference and pass it to the script as a parameter. The script can then read data from the data reference parameter just like a local file path.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Configure the data reference to be passed to the Estimator")]),t._v("\ndata_ref "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_download"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path_on_compute"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nestimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SKLearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("source_directory"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'experiment_folder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    entry_script"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_script.py'")]),t._v("\n                    compute_target"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    script_params "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data_folder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" data_ref"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Usage in the script")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" argparse\n\nparser "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" argparse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ArgumentParser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nparser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data_folder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dest"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_folder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nargs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata_files "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("listdir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_folder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"datasets"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#datasets"}},[t._v("#")]),t._v(" Datasets")]),t._v(" "),a("p",[t._v("They are versioned packaged data objects.\nThere are two different types:")]),t._v(" "),a("ol",[a("li",[t._v("Tabular (structured)")]),t._v(" "),a("li",[t._v("File (unstructured)")])]),t._v(" "),a("p",[t._v("You can create datasets from individual files or multiple file paths. The paths can include wildcards (for example, /files/*.csv) making it possible to encapsulate data from a large number of files in a single dataset.")]),t._v(" "),a("h3",{attrs:{id:"creating-a-tabular-dataset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-tabular-dataset"}},[t._v("#")]),t._v(" Creating a Tabular Dataset")]),t._v(" "),a("p",[t._v("Using the SDK, you can use the "),a("code",[t._v("from_delimited_files")]),t._v(" method in the "),a("code",[t._v("Dataset")]),t._v(" class.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\nblob_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_default_datastore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncsv_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/current_data.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/archive/*.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ntab_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tabular"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_delimited_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("csv_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntab_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tab_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv_table'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"creating-a-file-dataset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-a-file-dataset"}},[t._v("#")]),t._v(" Creating a File Dataset")]),t._v(" "),a("p",[t._v("Using the SDK, you can use the "),a("code",[t._v("from_files")]),t._v(" method in the "),a("code",[t._v("Dataset")]),t._v(" class.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" azureml"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset\n\nblob_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_default_datastore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" file_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("To retrieve a dataset you can do one of the following:")]),t._v(" "),a("ol",[a("li",[t._v("Use the "),a("code",[t._v("Datasets")]),t._v(" dictionary attribute of the Workspace object")]),t._v(" "),a("li",[t._v("Use the "),a("code",[t._v("get_by_name")]),t._v(" or "),a("code",[t._v("get_by_id")]),t._v(" of the "),a("code",[t._v("Dataset")]),t._v(" class.")])]),t._v(" "),a("h3",{attrs:{id:"reading-data-from-datasets"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reading-data-from-datasets"}},[t._v("#")]),t._v(" Reading Data from Datasets")]),t._v(" "),a("p",[t._v("Data can be read directly from a dataset or passed as a named input to a script configuration.")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Tabular")]),t._v("\ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tab_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_pandas_dataframe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Files")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" file_path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" file_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_path\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# As Scripts")]),t._v("\nestimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SKLearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" source_directory"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'experiment_folder'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     entry_script"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_script.py'")]),t._v("\n                     compute_target"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     inputs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tab_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("as_named_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'csv_data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     pip_packages"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'azureml-dataprep[pandas]'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("strong",[t._v("Note")]),t._v(" Since we are passing the dataset as an argument, we nned the "),a("code",[t._v("azureml-dataprep[pandas]")]),t._v(" package installed.")]),t._v(" "),a("h3",{attrs:{id:"data-versioning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-versioning"}},[t._v("#")]),t._v(" Data Versioning")]),t._v(" "),a("p",[t._v("Data can be versioned by using the "),a("code",[t._v("create_new_version")]),t._v(" argument in the "),a("code",[t._v("register")]),t._v(" function")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("img_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("blob_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data/files/images/*.png'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nfile_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("File"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_files"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfile_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" file_ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("register"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" create_new_version"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# To retrieve a specific version")]),t._v("\nimg_ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_by_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("workspace"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ws"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'img_files'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" version"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=n.exports}}]);