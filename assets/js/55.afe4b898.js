(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{413:function(t,e,a){"use strict";a.r(e);var r=a(42),n=Object(r.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"spark-basics"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-basics"}},[t._v("#")]),t._v(" Spark Basics")]),t._v(" "),a("p",[t._v("Notes taken for the Databricks Spark Developer Certification")]),t._v(" "),a("h2",{attrs:{id:"spark-basic-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-basic-architecture"}},[t._v("#")]),t._v(" Spark Basic Architecture")]),t._v(" "),a("h3",{attrs:{id:"spark-components"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-components"}},[t._v("#")]),t._v(" Spark Components")]),t._v(" "),a("p",[a("strong",[t._v("Driver")])]),t._v(" "),a("p",[t._v("Machine in which the application runs:")]),t._v(" "),a("ul",[a("li",[t._v("Mantains info about Spark application")]),t._v(" "),a("li",[t._v("Responds to user's program")]),t._v(" "),a("li",[t._v("Analyses, distributes, and schedules work across executors\nNote: In a single cluster, there will only be ONE driver, regardless of the num of executors.")])]),t._v(" "),a("p",[a("strong",[t._v("Executors")])]),t._v(" "),a("p",[t._v("Holds a chunk (partition) of data to be processed. Responsible for carrying out work assigned bt the driver")]),t._v(" "),a("ul",[a("li",[t._v("Executes code assigned by the driver")]),t._v(" "),a("li",[t._v("Reports the state of the computation back to the driver")])]),t._v(" "),a("p",[a("strong",[t._v("Slot")])]),t._v(" "),a("p",[t._v("Each executor has a number of slots, each slot can be assigned a task.")]),t._v(" "),a("p",[a("strong",[t._v("Task")])]),t._v(" "),a("p",[t._v("Tasks are created by the driver and assigned a partition of data to process.")]),t._v(" "),a("p",[a("strong",[t._v("Notes")])]),t._v(" "),a("p",[t._v("Driver => Worker => Executor => Slots")]),t._v(" "),a("h3",{attrs:{id:"transformations-actions-executions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformations-actions-executions"}},[t._v("#")]),t._v(" Transformations, Actions, Executions")]),t._v(" "),a("p",[a("strong",[t._v("Lazy Evaluation")]),t._v("\nSpark waits until the last moment to execute a series of operations.")]),t._v(" "),a("p",[a("strong",[t._v("Transformations")]),t._v("\nThe instructions you use to modify a DataFrame to get the results that you want")]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("Narrow Transformations")]),t._v(" the data required to compute the records in a single partition reside in at most one partition of the parent dataset.")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("Wide transformations")]),t._v(" the data required to compute the records in a single partition may reside in many partitions of the parent dataset. Wide transformations require that data be redistributed over the system. This is called a shuffle. Shuffles are triggered when data needs to move between executors.")])])]),t._v(" "),a("p",[a("strong",[a("code",[t._v("Narrow")]),t._v(" vs "),a("code",[t._v("Wide")]),t._v(" transformations")])]),t._v(" "),a("p",[t._v("Any function that results in shuffling is a "),a("code",[t._v("wide")]),t._v(" transformation.")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("Type")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Definition")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("Examples")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[a("strong",[t._v("Narrow")])]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Those where each input partition will contribute to only one output partition.")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[a("code",[t._v("select")]),t._v(", "),a("code",[t._v("filter")]),t._v(", "),a("code",[t._v("withColumn")]),t._v(","),a("code",[t._v("drop")])])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[a("strong",[t._v("Wide")])]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("Have input partitions contributing to many output partitions")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[a("code",[t._v("distinct")]),t._v(", "),a("code",[t._v("union")]),t._v(", "),a("code",[t._v("groupBy")]),t._v(", "),a("code",[t._v("sort")]),t._v(", "),a("code",[t._v("join")])])])])]),t._v(" "),a("p",[a("strong",[t._v("Cluster Manager")])]),t._v(" "),a("p",[a("strong",[t._v("Dynamic Partition Prunning")]),t._v("\nIs intended to skip over the data you don't need in the results of a query")]),t._v(" "),a("p",[a("strong",[t._v("Spark Garbage Collection")])]),t._v(" "),a("ul",[a("li",[t._v("Garbagae collection information can be accessed in Spark's UI")]),t._v(" "),a("li",[t._v("Serializing caching can be a strategy to increase the performance of GC")]),t._v(" "),a("li",[t._v("Optimizing garbage collection performance in Spark may limit caching ability.")])]),t._v(" "),a("p",[a("strong",[t._v("Client vs Cluster Mode")]),t._v("\nIn cluster mode, the driver runs on the worker nodes, while in client mode, the driver runs on the client machine.")]),t._v(" "),a("p",[a("strong",[a("a",{attrs:{href:"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adaptive Query Execution"),a("OutboundLink")],1)]),t._v("\nFeatures are dynamically switching join strategies and dynamically optimizing skew joins")])])}),[],!1,null,null,null);e.default=n.exports}}]);